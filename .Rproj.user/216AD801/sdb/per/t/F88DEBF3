{
    "contents" : "```{r global_options, include= FALSE}\noptions(digits= 7)\n```\n\n## A Predictive Model for the Weight Lifting Exercises Dataset\n\n### Synopsis\n\nIn this project, we construct a predictive classification model based on the Weight Lifting Exercises dataset.\n\nThe training data used for this project is available at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv, and the test data is available at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv. Information about this data can be find at http://groupware.les.inf.puc-rio.br/har.\n\nThe model classifies how a weight lifting exercise was executed: Exactly according to the specification (class A), or with with a common mistake type (classes B-E).\n\n### Reading and partitioning the data\n\n```{r}\nlibrary(caret)\nset.seed(123)\n\nlabeled    <- read.csv(\"pml-training.csv\") # training+cv+test data\nunlabeled  <- read.csv(\"pml-testing.csv\" ) # unlabeled test   data\n```\n\nThe labeled data contains `r dim(labeled)[1]` cases, and `r dim(labeled)[2]` variables (`r dim(labeled)[2]-1` predictors, and the true class). The unlabeled data contains `r dim(unlabeled)[1]` cases.\n\nWe'll examine the proportion of the different labels:\n\n```{r}\nprop.table(table(labeled$classe))\n```\n\nWe can see that each label has a decent representation in the labeled set.\n\nWe'll keep 10% of the labeled data for testing, and use 90% for training/cross-validation (see motivation in \"Building the model\" below). We'll random-split the labeled data, stratified by the label:\n\n```{r}\ninTrain    <- createDataPartition(y= labeled$classe, p= 0.9, list= F)\ntraining   <- labeled[ inTrain,]\ntest       <- labeled[-inTrain,]\n```\n\n### Selecting and preprocessing predictors\n\nFirst we will remove the index column (which is not a predictor)\n\n```{r}\ntraining$X <- NULL\ntest$X     <- NULL\nunlabeled$X<- NULL\n```\n\nNext, we will remove predictors where more than 97% of the values are NA:\n\n```{r}\npercentNA  <- as.vector(colMeans(is.na(training)))\ncolsNA     <- which(percentNA > 0.97)\ntraining   <- training [, -colsNA]\ntest       <- test     [, -colsNA]\nunlabeled  <- unlabeled[, -colsNA]\n```\n\nWe're now left with `r dim(training)[2]-1` predictors.   \n\nNext, we'll remove all near-zero variance predictors:\n\n```{r}\nnsv        <- nearZeroVar(training)\ntraining   <- training [, -nsv]\ntest       <- test     [, -nsv]\nunlabeled  <- unlabeled[, -nsv]\n```\n\nWe're now left with `r dim(training)[2]-1` predictors.\n\nWe will further reduce the number of predictors using Principal Component Analysis on all non-factor predictors, retaining 99% of the variance:\n\n```{r}\ncolsFactor <- as.vector(which(sapply(training, is.factor)))\npreProc    <- preProcess(training[, -colsFactor], method= \"pca\", thresh= 0.99)\n\ntraining   <- cbind(predict(preProc, training [, -colsFactor]), training [colsFactor])\ntest       <- cbind(predict(preProc, test     [, -colsFactor]), test     [colsFactor])\nunlabeled  <- cbind(predict(preProc, unlabeled[, -colsFactor]), unlabeled[colsFactor])\n```\n\nWe're now left with `r dim(training)[2]-1` predictors.\n\n### Building the model\n\nWe'll train a random forest model using 10-fold cross validation. The training process optimizes the mtry parameter (number of variables randomly selected at each tree node). \n\n```{r}\nfitControl <- trainControl(method= \"cv\", number= 10, repeats= 1, verboseIter= F)\nmodFit     <- train(classe ~ ., method= \"rf\" , data= training, trControl= fitControl)\n```\n\nSince the validation folds are used to select the final model (best mtry value), the cross-validation error rate estimate may be biased. \n\nWe'll evaluate the accuracy both for the training/cv set and for the test set:\n\n### Evaluating the model: training/cross-validation set\n\nNote: As stated above: this evaluation may be biased.\n\n```{r}\nmodFit\nAccuracyM  <- max(modFit$results$Accuracy)\n```\n\nBased on the 10-folds cross validation results, the estimated accuracy of the model is `r AccuracyM*100`%, with a tight confidence interval. Hence, the out-of-sample error is `r (1-AccuracyM)*100`%\n\nThis estimated model accuracy is the average accuracy for the 10 cross-validation folds:\n\n```{r}\nmodFit$resample\nmean(modFit$resample$Accuracy)\n```\n\nRunning the model on the training/cv set produce the following results:\n\n```{r}\np1         <- predict(modFit, training)\nm1         <- confusionMatrix(p1, training$classe)\nm1\nAccuracy1  <- m1$overall[\"Accuracy\"]\n```\n\nThe accuracy for our model for the training data is `r Accuracy1*100`% with a tight confidence interval. \n\n### Evaluating the model: test set\n\nNow we'll evaluate the [unbiased] accuracy estimation of our model - based on the test set:\n\n```{r}\np2         <- predict(modFit, test)\nm2         <- confusionMatrix(p2, test$classe)\nm2\nAccuracy2  <- m2$overall[\"Accuracy\"]\n```\n\nBased on the test set, the estimated accuracy of our model is `r Accuracy2*100`%, with a tight confidence interval. Hence, the out-of-sample error is `r (1-Accuracy2)*100`%.\n\nThe unbiased error estimation turns out to be very similar to the cross-validation error.\n\n### Prediction for the unlabeled set\n\nWe will now predict the label for our unlabeled test data:\n\n```{r}\npredict(modFit, unlabeled)\n```\n",
    "created" : 1405874315027.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1808483667",
    "id" : "F88DEBF3",
    "lastKnownWriteTime" : 1406396203,
    "path" : "C:/My GitHub/Practical-Machine-Learning/WLE.Rmd",
    "project_path" : "WLE.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}